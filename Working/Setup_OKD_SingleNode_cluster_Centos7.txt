Setup Local OpenShift Origin (OKD 3.x) Cluster on CentOS 7
-----------------------------
Note**For OKD 4.x https://github.com/openshift/okd/blob/master/README.md#getting-started

Good to have
8 vCPUs
32 GB RAM
50 GB free disc space
CentOS 7 OS

#Update CentOS 7 system
sudo yum -y update

#Install and Configure Docker
#OpenShift required docker engine on the host machine for running containers. Install Docker on CentOS 7 using the commands below.
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install -y  docker-ce docker-ce-cli containerd.io

#Add your standard user account to docker group
sudo usermod -aG docker $USER
newgrp docker

#After installing Docker, configure the Docker daemon with an insecure registry parameter of 172.30.0.0/16
sudo mkdir /etc/docker /etc/containers

sudo tee /etc/containers/registries.conf<<EOF
[registries.insecure]
registries = ['172.30.0.0/16']
EOF

sudo tee /etc/docker/daemon.json<<EOF
{
   "insecure-registries": [
     "172.30.0.0/16"
   ]
}
EOF

# reload systemd and restart the Docker daemon after editing the config
sudo systemctl daemon-reload
sudo systemctl restart docker

#Enable Docker to start at boot
sudo systemctl enable docker
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.

#Then enable IP forwarding on your system
echo "net.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

#Configure Firewalld

#Ensure that your firewall allows containers access to the OpenShift master API (8443/tcp) and DNS (53/udp) endpoints

DOCKER_BRIDGE=`docker network inspect -f "{{range .IPAM.Config }}{{ .Subnet }}{{end}}" bridge`
#sudo firewall-cmd --permanent --new-zone dockerc
optionally if required: sudo firewall-cmd --permanent --delete-zone dockerc
#sudo firewall-cmd --permanent --zone dockerc --add-source $DOCKER_BRIDGE
#sudo firewall-cmd --permanent --zone dockerc --add-port={80,443,8443}/tcp
#sudo firewall-cmd --permanent --zone dockerc --add-port={53,8053}/udp
#sudo firewall-cmd --reload

===============
#The above mention dockerc zone may give a problem later, thus do this..
firewall-cmd --permanent --delete-zone dockerc
firewall-cmd --permanent --new-zone dockerc
firewall-cmd --permanent --zone dockerc --add-source 172.17.0.0/16
firewall-cmd --permanent --zone dockerc --add-port 8443/tcp
firewall-cmd --permanent --zone dockerc --add-port 53/udp
firewall-cmd --permanent --zone dockerc --add-port 8053/udp
firewall-cmd --reload
=================
#From openshift directory
./oc cluster up --config openshift.local.clusterup/openshift-controller-manager/openshift-master.kubeconfig 
Getting a Docker client ...
Checking if image openshift/origin-control-plane:v3.11 is available ...
Checking type of volume mount ...
Determining server IP ...
Checking if OpenShift is already running ...
Checking for supported Docker version (=>1.22) ...
Checking if insecured registry is configured properly in Docker ...
Checking if required ports are available ...
Checking if OpenShift client is configured properly ...
Checking if image openshift/origin-control-plane:v3.11 is available ...
Starting OpenShift using openshift/origin-control-plane:v3.11 ...
I0305 11:02:21.081932   31214 flags.go:30] Running "create-kubelet-flags"
I0305 11:02:21.591760   31214 run_kubelet.go:49] Running "start-kubelet"
I0305 11:02:21.776167   31214 run_self_hosted.go:181] Waiting for the kube-apiserver to be ready ...
I0305 11:02:50.817188   31214 interface.go:26] Installing "kube-proxy" ...
I0305 11:02:50.817200   31214 interface.go:26] Installing "kube-dns" ...
I0305 11:02:50.817206   31214 interface.go:26] Installing "openshift-service-cert-signer-operator" ...
I0305 11:02:50.817212   31214 interface.go:26] Installing "openshift-apiserver" ...
I0305 11:02:50.817234   31214 apply_template.go:81] Installing "openshift-apiserver"
I0305 11:02:50.817358   31214 apply_template.go:81] Installing "kube-proxy"
I0305 11:02:50.817465   31214 apply_template.go:81] Installing "kube-dns"
I0305 11:02:50.817601   31214 apply_template.go:81] Installing "openshift-service-cert-signer-operator"
I0305 11:03:01.255616   31214 interface.go:41] Finished installing "kube-proxy" "kube-dns" "openshift-service-cert-signer-operator" "openshift-apiserver"
I0305 11:04:36.320785   31214 run_self_hosted.go:242] openshift-apiserver available
I0305 11:04:36.320822   31214 interface.go:26] Installing "openshift-controller-manager" ...
I0305 11:04:36.320848   31214 apply_template.go:81] Installing "openshift-controller-manager"
I0305 11:04:41.467554   31214 interface.go:41] Finished installing "openshift-controller-manager"
Adding default OAuthClient redirect URIs ...
Adding registry ...
Adding persistent-volumes ...
Adding web-console ...
Adding sample-templates ...
Adding centos-imagestreams ...
Adding router ...
I0305 11:04:41.508432   31214 interface.go:26] Installing "openshift-image-registry" ...
I0305 11:04:41.508440   31214 interface.go:26] Installing "persistent-volumes" ...
I0305 11:04:41.508446   31214 interface.go:26] Installing "openshift-web-console-operator" ...
I0305 11:04:41.508451   31214 interface.go:26] Installing "sample-templates" ...
I0305 11:04:41.508456   31214 interface.go:26] Installing "centos-imagestreams" ...
I0305 11:04:41.508460   31214 interface.go:26] Installing "openshift-router" ...
I0305 11:04:41.509816   31214 apply_template.go:81] Installing "openshift-web-console-operator"
I0305 11:04:41.509913   31214 interface.go:26] Installing "sample-templates/cakephp quickstart" ...
I0305 11:04:41.509921   31214 interface.go:26] Installing "sample-templates/dancer quickstart" ...
I0305 11:04:41.509926   31214 interface.go:26] Installing "sample-templates/django quickstart" ...
I0305 11:04:41.509931   31214 interface.go:26] Installing "sample-templates/jenkins pipeline ephemeral" ...
I0305 11:04:41.509937   31214 interface.go:26] Installing "sample-templates/sample pipeline" ...
I0305 11:04:41.509941   31214 interface.go:26] Installing "sample-templates/mongodb" ...
I0305 11:04:41.509947   31214 interface.go:26] Installing "sample-templates/mariadb" ...
I0305 11:04:41.509951   31214 interface.go:26] Installing "sample-templates/mysql" ...
I0305 11:04:41.509964   31214 interface.go:26] Installing "sample-templates/postgresql" ...
I0305 11:04:41.509969   31214 interface.go:26] Installing "sample-templates/nodejs quickstart" ...
I0305 11:04:41.509974   31214 interface.go:26] Installing "sample-templates/rails quickstart" ...
I0305 11:04:41.509997   31214 apply_list.go:67] Installing "sample-templates/rails quickstart"
I0305 11:04:41.510087   31214 apply_list.go:67] Installing "centos-imagestreams"
I0305 11:04:41.510164   31214 apply_list.go:67] Installing "sample-templates/cakephp quickstart"
I0305 11:04:41.510224   31214 apply_list.go:67] Installing "sample-templates/dancer quickstart"
I0305 11:04:41.510307   31214 apply_list.go:67] Installing "sample-templates/django quickstart"
I0305 11:04:41.510366   31214 apply_list.go:67] Installing "sample-templates/jenkins pipeline ephemeral"
I0305 11:04:41.510423   31214 apply_list.go:67] Installing "sample-templates/sample pipeline"
I0305 11:04:41.510483   31214 apply_list.go:67] Installing "sample-templates/mongodb"
I0305 11:04:41.510541   31214 apply_list.go:67] Installing "sample-templates/mariadb"
I0305 11:04:41.510602   31214 apply_list.go:67] Installing "sample-templates/mysql"
I0305 11:04:41.510661   31214 apply_list.go:67] Installing "sample-templates/postgresql"
I0305 11:04:41.510784   31214 apply_list.go:67] Installing "sample-templates/nodejs quickstart"
I0305 11:05:03.423381   31214 interface.go:41] Finished installing "sample-templates/cakephp quickstart" "sample-templates/dancer quickstart" "sample-templates/django quickstart" "sample-templates/jenkins pipeline ephemeral" "sample-templates/sample pipeline" "sample-templates/mongodb" "sample-templates/mariadb" "sample-templates/mysql" "sample-templates/postgresql" "sample-templates/nodejs quickstart" "sample-templates/rails quickstart"
I0305 11:05:44.233237   31214 interface.go:41] Finished installing "openshift-image-registry" "persistent-volumes" "openshift-web-console-operator" "sample-templates" "centos-imagestreams" "openshift-router"
Login to server ...
Creating initial project "myproject" ...
Error: invalid configuration: Missing or incomplete configuration info.  Please login or point to an existing, complete config file:

  1. Via the command-line flag --config
  2. Via the KUBECONFIG environment variable
  3. In your home directory as ~/.kube/config

To view or setup config directly use the 'config' command.
[root@c5 openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit]# ./oc login -u system:admin --config openshift.local.clusterup/openshift-controller-manager/openshift-master.kubeconfig 
Logged into "https://127.0.0.1:8443" as "system:admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-dns
    kube-proxy
    kube-public
    kube-system
    openshift
    openshift-apiserver
    openshift-controller-manager
    openshift-core-operators
    openshift-infra
    openshift-node
    openshift-service-cert-signer
    openshift-web-console

Using project "default".
[root@c5 openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit]# 

===============
#Download the Linux oc binary
#we can download the Linux oc binary from openshift-origin-client-tools-VERSION-linux-64bit.tar.gz and place it in your path

wget https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz
tar xvf openshift-origin-client-tools*.tar.gz
root@c5:cd openshift-origin-client*/

#Verify installation of OpenShift client utility
$./oc version
oc v3.11.0+0cbc58b
kubernetes v1.11.0+d4cacc0
features: Basic-Auth GSSAPI Kerberos SPNEGO

#Start OpenShift Origin (OKD) Local Cluster
Now bootstrap a local single server OpenShift Origin cluster by running the following command
$./oc cluster up

The command above will:

Start OKD Cluster listening on the local interface – 127.0.0.1:8443
Start a web console listening on all interfaces at /console (127.0.0.1:8443).
Launch Kubernetes system components.
Provisions registry, router, initial templates, and a default project.
The OpenShift cluster will run as an all-in-one container on a Docker host.
There are a number of options which can be applied when setting up 
Openshift Origin, view them with:

$./oc cluster up --help

#On a successful installation, you should get output similar to below
Login to server …
Creating initial project "myproject" …
Server Information …
OpenShift server started.
The server is accessible via web console at:
     https://127.0.0.1:8443
You are logged in as:
     User:     developer
     Password: <any value>
To login as administrator:
     oc login -u system:admin

#Example below uses custom options.
$./oc cluster up --routing-suffix=<ServerPublicIP>.xip.io \
 --public-hostname=<ServerPulicDNSName>

For example:
$./oc cluster up --public-hostname=okd.example.com --routing-suffix='services.example.com'

#The OpenShift Origin cluster configuration files will be located inside the 
openshift.local.clusterup/ directory.

#If your cluster setup was successful, you should get a positive 
output for the following command.

$./oc cluster status
Web console URL: https://okd.example.com:8443/console/

Config is at host directory 
Volumes are at host directory 
Persistent volumes are at host directory 
/home/dev/openshift.local.clusterup/openshift.local.pv
Data will be discarded when cluster is destroyed

#Using OpenShift Origin Cluster
To login as an administrator, use:
$./oc login -u system:admin
Logged into "https://127.0.0.1:8443" as "system:admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-dns
    kube-proxy
    kube-public
    kube-system
    myproject
    openshift
    openshift-apiserver
    openshift-controller-manager
    openshift-core-operators
    openshift-infra
    openshift-node
    openshift-service-cert-signer
    openshift-web-console
    testproject

Using project "default".

#As System Admin user, you can few information such as node status.
$./oc get nodes
NAME        STATUS    ROLES     AGE       VERSION
localhost   Ready     <none>    1h        v1.11.0+d4cacc0

$./oc get nodes -o wide

#To get more detailed information about a specific node, 
including the reason for the current condition:
$./oc describe node <node>

#To display a summary of the resources you created:
$./oc status
In project default on server https://127.0.0.1:8443

svc/docker-registry - 172.30.1.1:5000
  dc/docker-registry deploys docker.io/openshift/origin-docker-registry:v3.11 
    deployment #1 deployed 2 hours ago - 1 pod

svc/kubernetes - 172.30.0.1:443 -> 8443

svc/router - 172.30.235.156 ports 80, 443, 1936
  dc/router deploys docker.io/openshift/origin-haproxy-router:v3.11 
    deployment #1 deployed 2 hours ago - 1 pod

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.

#To return to the regular developer user, login as that user:
$./oc login
Authentication required for https://127.0.0.1:8443 (openshift)
Username: developer
Password: developer
Login successful.

#Confirm if Login was successful.
$./oc whoami
developer

#create a test project using oc new-project command.
$./oc new-project dev --display-name="Project1 - Dev" \
   --description="My Dev Project"

Now using project "dev" on server "https://127.0.0.1:8443"

#Using OKD Admin Console
OKD includes a web console which you can use for creation and management actions. This web console is accessible on Server IP/Hostname on the port,8443 via https.

https://<IP|Hostname>:8443/console
#You should see an OpenShift Origin window with Username and Password forms
Username: developer
Password: developer
===================

See if you see a dashboard
If you are redirected to https://127.0.0.1:8443/ when trying to access OpenShift web console, then do this:
1. Stop OpenShift Cluster
$./oc cluster down

2. Edit OCP configuration file.
$ vi ./openshift.local.clusterup/openshift-controller-manager/openshift-master.kubeconfig

Locate line “server: https://127.0.0.1:8443“, then replace with:
server: https://serverip:8443

3. Then start cluster:
$./oc cluster up

Deploy Test Application
--------------------
1. Login to Openshift cluster:
$./oc login 
Authentication required for https://https://127.0.0.1:8443 (openshift)
Username: developer 
Password: developer
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project 

2. Create a test project.
$./oc new-project test-project 

3. Tag an application image from Docker Hub registry.
$./oc tag --source=docker openshift/deployment-example:v2 deployment-example:latest 
Tag deployment-example:latest set to openshift/deployment-example:v2.

4. Deploy Application to OpenShift.
$./oc new-app deployment-example 
--> Found image da61bb2 (3 years old) in image stream "test-project/deployment-example" under tag "latest" for "deployment-example"

    * This image will be deployed in deployment config "deployment-example"
    * Port 8080/tcp will be load balanced by service "deployment-example"
      * Other containers can access this service through the hostname "deployment-example"
    * WARNING: Image "test-project/deployment-example:latest" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources ...
    deploymentconfig.apps.openshift.io "deployment-example" created
    service "deployment-example" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/deployment-example'
    Run 'oc status' to view your app.

5. Show Application Deployment status.
$./oc status
In project test-project on server https://127.0.0.1:8443

svc/deployment-example - 172.30.15.201:8080
  dc/deployment-example deploys istag/deployment-example:latest 
    deployment #1 deployed about a minute ago - 1 pod

2 infos identified, use 'oc status --suggest' to see details.

6. Get service detailed information.
$./oc get svc
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
deployment-example   ClusterIP   172.30.15.201           8080/TCP   18m

$./oc describe svc deployment-example
Name:              deployment-example
Namespace:         test-project
Labels:            app=deployment-example
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=deployment-example,deploymentconfig=deployment-example
Type:              ClusterIP
IP:                172.30.15.201
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.12:8080
Session Affinity:  None
Events:            <none>

7. Test App local access.
curl http://172.30.15.201:8080

8. Show Pods status
$./oc get pods
NAME                         READY     STATUS    RESTARTS   AGE
deployment-example-1-vmf7t   1/1       Running   0          21m

9. Allow external access to the application.

$./oc expose service/deployment-example
route.route.openshift.io/deployment-example exposed

$./oc get routes
NAME                 HOST/PORT                                                       PATH      SERVICES             PORT       TERMINATION   WILDCARD
deployment-example   deployment-example-testproject.services.computingforgeeks.com             deployment-example   8080-tcp                 None

10. Test external access to the application.
Open the URL shown in your browser.














